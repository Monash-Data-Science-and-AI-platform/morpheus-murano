{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Morpheus Murano App 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Murano App uses a [microk8s cluster](https://microk8s.io/) to provide a space to test NVIDIA Morpheus on the Nectar cloud. To seperate Morpheus from any other deployments you may wish to test, these deployments use the `morpheus` namespace. \n",
    "\n",
    "This app includes a jupyter lab server to make things more user friendly during experimentation and NginX to make accessing it and the MLFlow UI easier. Jupyter lab is running in the `jupyter` tmux session. If this server shuts down for whatever reason, logging into the instance via ssh should reboot the server when the `.bashrc` script runs.\n",
    "\n",
    "The morpheus stack includes:\n",
    "\n",
    "- [MLFlow](https://mlflow.org/) to manage models\n",
    "- [Triton](https://developer.nvidia.com/nvidia-triton-inference-server) as an inference server\n",
    "- [Kafka](https://kafka.apache.org/) as a data broker\n",
    "\n",
    "You can check the health of the cluster using:\n",
    "`microk8s kubectl -n morpheus get all`\n",
    "\n",
    "While MLFlow does come with a UI, we have not made it available since there is no easy way to protect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'microk8s' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!microk8s kubectl -n morpheus get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to wait for Morpheus to finish initialising. This will be signified by all pods having the Running status. (Approx. 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'while' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "\n",
    "prev = [0]\n",
    "running = 0\n",
    "\n",
    "# Use the space used up on the disk as a proxy for measuring the download progress\n",
    "with tqdm(total=63000000) as pbar:\n",
    "    # Continue until all 5 containers are up\n",
    "    while int(running[0]) < 5:\n",
    "        # Get the current disk usage and update according to download\n",
    "        curr = !df | grep / dev/vda1 | awk '{print $3}'\n",
    "        pbar.update(int(curr[0]) - int(prev[0]))\n",
    "        prev = curr\n",
    "        sleep(1)\n",
    "        \n",
    "        # Check how many pods are up\n",
    "        running = !microk8s kubectl get pods -n morpheus | grep -c 'Running'\n",
    "        try: \n",
    "            running = int(running[0])\n",
    "        except:\n",
    "            running = 0\n",
    "\n",
    "\n",
    "pbar.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on usage, have a read of the [quick start guide](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html) which this notebook is based on.\n",
    "\n",
    "There are two key differences between the quick start guide and this notebook. \n",
    "\n",
    "1. This app uses microk8s, so we will need to prepend `microk8s` before every `kubectl` command\n",
    "2. We have reimplemented the guide as a notebook to make create a more user friendly introduction. Morpheus was not made with notebooks in mind, so there are some things that we've had to do in order to run things in notebook shells rather than do things interactively in the containers. (Note the aliases below) \n",
    "\n",
    "NVIDIA recommends that you only run one pipeline at once, this means that usually you would need to uninstall any active pipelines before replacing it during testing. However, we are working with the SDK through this notebook. By default this app is running the the `Morpheus SDK Client` in 'sleep mode' under the `helper` release name. Rather than destroying and recreating the sdk, we will be interfacing with it through the CLI directly, building pipelines with the CLI. Simply stop the running cell and make edits as desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Morpheus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morpheus can be run in two ways:\n",
    "\n",
    "- Run off file\n",
    "  \n",
    "    NVIDIA provides several example models and datasets to test morpheus with. You can browse these on their [GitHub](https://github.com/nv-morpheus/Morpheus/tree/branch-22.06/models). The quick start guide includes several examples which involve loading in this data into the input topics and storing outputs in a file. You can also upload your own data to the container using Jupyter's interface.\n",
    "\n",
    "- Run off kafka topic\n",
    "  \n",
    "    NVIDIA Morpheus can interface with the kafka data broker, pulling new data from an input topic and pushing inferences to an output topic. NVIDIA provides scripts to load data from a file into the input topic to simulate a data stream, but in production this should be gathered from the system you are monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of Morpheus is deployed using five containers:\n",
    "\n",
    "- The *ai-engine* container, which runs NVIDIA Triton, listening for HTTP requests on port 8000, gRPC on port 8001 and metrics in the prometheus format on port 8002.\n",
    "- The *sdk-cli-helper* container, which runs the SDK and the Jupyter Lab (this container). We are using NginX to manage the connections to Jupyter to make your life easier.\n",
    "- The *broker* container, which runs Kafka, listening on port 9092. It is also exposed on 30092 on the main machine allowing you to feed it data, though it will be blocked by default by Nectar's security. You will need to enable this security rule if you'd like to do this, ensure that you are not working with sensitive data if you'd like to experiment with this.\n",
    "- The *zookeeper* which is a dependency of Kafka. It is used for synchronisation within distributed systems.\n",
    "- The *mlflow* container, which runs MLFlow, listening on port 5000. It's UI is also exposed on 30500 on the main machine allowing you to view the models you have deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up some aliases to execute commands in the containers to make our lives easier\n",
    "\n",
    "# Execute a command on the sdk pod\n",
    "x_sdk = \"microk8s kubectl -n morpheus exec -it sdk-cli-helper -- \"\n",
    "\n",
    "# Execute a command with the morpheus SDK\n",
    "x_morpheus = \"microk8s kubectl -n morpheus exec -it sdk-cli-helper -- /opt/conda/envs/morpheus/bin/morpheus\"\n",
    "\n",
    "# Run a command on the broker pod\n",
    "x_broker = \"microk8s kubectl -n morpheus exec deploy/broker -c broker -- \"\n",
    "\n",
    "# Run a command using MLFlow's python instance\n",
    "x_mlflow_python = \"microk8s kubectl -n morpheus exec -it deploy/mlflow -- /opt/conda/envs/mlflow/bin/python\"\n",
    "\n",
    "# Run a command with the MLFlow SDK\n",
    "x_mlflow = \"microk8s kubectl -n morpheus exec -it deploy/mlflow -- /opt/conda/envs/mlflow/bin/mlflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Information Detection (SID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA has provided us with a set of example models and datasets to get started in the `/models` and `/models/datasets` directory of the SDK container. In this example we will look at the SID model.\n",
    "\n",
    "To share these files with MLFlow, copy it to the `/common` directory which is mapped to `/opt/morpheus/common` on the host. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_sdk cp -RL /workspace/models /common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLFlow service is used for managing and deploying models. We can use NVIDIA's scripts to deploy models to the Triton ai-engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_mlflow_python publish_model_to_mlflow.py \\\n",
    "      --model_name sid-minibert-onnx \\\n",
    "      --model_directory /common/models/triton-model-repo/sid-minibert-onnx \\\n",
    "      --flavor triton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_mlflow deployments create -t triton \\\n",
    "      --flavor triton \\\n",
    "      --name sid-minibert-onnx \\\n",
    "      -m models:/sid-minibert-onnx/1 \\\n",
    "      -C \"version=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Our First Pipeline\n",
    "Now that we've finished setting up, we can set up some pipelines! Morpheus constructs pipelines made up of 'stages', including preprocessing and postprocessing steps accellerated with NVIDIA RAPIDS, which will allow you to build something to handle your data stream in near real time. If Morpheus's provided pipeline stages do not fit your needs, it also allows you to extend its capabilities with a [custom stage written in Python or C++](https://docs.nvidia.com/morpheus/developer_guide/guides/1_simple_python_stage.html#background).\n",
    "\n",
    "Morpheus provides scripts to simulate an input stream from a file or by streaming a file into the data broker, but in a production setting you would feed your data into the pipeline using the input topic we set up earlier.\n",
    "\n",
    "For more examples of these pipelines have a read through these [example workflows](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html#example-workflows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_sdk ls examples/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a datastream using the pcap_dump dataset file\n",
    "!$x_morpheus --log_level=DEBUG run \\\n",
    "      --num_threads=3 \\\n",
    "      --edge_buffer_size=4 \\\n",
    "      --use_cpp=True \\\n",
    "      --pipeline_batch_size=1024 \\\n",
    "      --model_max_batch_size=32 \\\n",
    "      pipeline-nlp \\\n",
    "        --model_seq_length=256 \\\n",
    "        from-file --filename=./examples/data/pcap_dump.jsonlines \\\n",
    "        monitor --description 'FromFile Rate' --smoothing=0.001 \\\n",
    "        deserialize \\\n",
    "        preprocess --vocab_hash_file=data/bert-base-uncased-hash.txt --truncation=True --do_lower_case=True --add_special_tokens=False \\\n",
    "        monitor --description='Preprocessing rate' \\\n",
    "        inf-triton --force_convert_inputs=True --model_name=sid-minibert-onnx --server_url=ai-engine:8001 \\\n",
    "        monitor --description='Inference rate' --smoothing=0.001 --unit inf \\\n",
    "        add-class \\\n",
    "        serialize --exclude '^ts_' \\\n",
    "        to-file --filename=/common/data/sid-minibert-onnx-output.jsonlines --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause here and examine what's printed out in the console. \n",
    "\n",
    "Morpheus needs a 'labels' file to map inferences from a class to a human readable label. If one is not provided, it will look in the default location. \n",
    "\n",
    "Next Morpheus outputs its config: it will report the type of pipeline it's using (NLP) and the parameters we've chosen for inference (batch size, threads etc)\n",
    "\n",
    "Then Morpheus will output its progress in building the pipeline, this output will let you know which stage has failed if the pipeline cannot be built for whatever reason.\n",
    "\n",
    "Finally the will begin processing. Since we've asked the pipeline to monitor the rate of processing the data, we see these outputs here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A closer look\n",
    "At this stage you may want to explore the containers to get a sense of the tools available. Most of the SDKs and scripts will have a help page explaining their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at the files on the SDK container\n",
    "!$x_sdk ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make GET requests from the SDK container to the ai-engine\n",
    "# Check ai-engine (triton) is ready for requests\n",
    "!$x_sdk curl -v ai-engine:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The morpheus SDK help page\n",
    "!$x_morpheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one of NVIDIA's shell scripts in the broker pod\n",
    "!$x_broker kafka-topics.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MLFlow SDK help page\n",
    "!$x_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way to run a pipeline with Morpheus is to connect it to kafka topics for input and output. To do this we will need to create these topics using one of NVIDIA's provided scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input topic\n",
    "!$x_broker kafka-topics.sh\\\n",
    "      --create \\\n",
    "      --bootstrap-server broker:9092 \\\n",
    "      --replication-factor 1 \\\n",
    "      --partitions 3 \\\n",
    "      --topic input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output topic\n",
    "!$x_broker kafka-topics.sh\\\n",
    "      --create \\\n",
    "      --bootstrap-server broker:9092 \\\n",
    "      --replication-factor 1 \\\n",
    "      --partitions 3 \\\n",
    "      --topic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the topics we've just created\n",
    "!$x_broker --list  --zookeeper zookeeper:2181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you could write a pipeline to ingest data from the input topic and publish inferences to the . The easiest way to test this pipeline would be to use the `kafka-console-producer.sh` script in the broker container to simulate an input stream. There are several examples of how to do this in the [quick start guide](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html#run-nlp-sensitive-information-detection-pipeline).\n",
    "\n",
    "Since the stream will need to run at the same time as the pipeline you will likely need to open up a terminal to do this. We have left this as an exercise for you if you'd like to explore this feature. Alternatively you could connect these kafka topics to another program which generates data - note that you will likely need to adjust the preprocessing steps so that they match your deployed model's expected format. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Using NVIDIA's pretrained models and examples are a good place to start to get a grasp of the framework, but fine tuning is necessary for any model deployed in a production setting.\n",
    "\n",
    "NVIDIA has provided training scripts and notebooks for each of their models for you to play around with to retrain their models. If their models do not adapt well to your use case you can also look at [hugging face's model repository](https://huggingface.co/models) as a place to start and train your own model.\n",
    "\n",
    "### WARNING\n",
    "Note that many of these scripts and notebooks were moved from their original repositories when being compiled into the Morpheus container. This means that any paths present within these scripts and notebooks **will not accurately reflect where they are in the container**. NVIDIA also **has not provided any environment files** to help us recreate the environments used to train these models. The datasets will likely be somewhere in the folder we copied to `/opt/morpheus/common/` earlier. It may be easier to browse the file structure on the [NVIDIA Morpheus GitHub](https://github.com/nv-morpheus/Morpheus/tree/branch-22.09/models). It will be up to you track down the datasets and recreate the environments if you wish to use the tools that NVIDIA have provided as a starting ground. Alternatively you can look through these resources to get inspiration on how you can train your own models to make use of this framework. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy NVIDIA's training scripts into our root directory so that we can easily access their scripts and notebooks\n",
    "!cp /opt/morpheus/common/models/training-tuning-scripts/ ./ -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "628da563ae30e285400e78415b7ce696a0a3188121a8a2ad023c6c9c424b5215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
