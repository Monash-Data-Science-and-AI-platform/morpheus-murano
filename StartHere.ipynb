{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Morpheus Murano App 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Murano App uses a microk8s cluster to provide a space to test NVIDIA Morpheus on the Nectar cloud. To seperate Morpheus from any other deployments you may wish to test, all the `morpheus` namespace has been used. \n",
    "\n",
    "This app includes a jupyter lab server to make things more user friendly during experimentation and NginX to make accessing it and the MLFlow UI easier.\n",
    "\n",
    "The morpheus stack includes:\n",
    "\n",
    "- [MLFlow](https://mlflow.org/) to manage models\n",
    "- [Triton](https://developer.nvidia.com/nvidia-triton-inference-server) as an inference server\n",
    "- [Kafka](https://kafka.apache.org/) as a data broker\n",
    "\n",
    "You can check the health of the cluster using:\n",
    "`microk8s kubectl -n morpheus get all`\n",
    "\n",
    "You can access the Jupyter Lab at `{address}/`\n",
    "You can access the MLFlow UI at `{address}/mlflow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!microk8s kubectl -n morpheus get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on usage, have a read of the [quick start guide](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html) which this notebook is based on.\n",
    "\n",
    "NVIDIA recommends that you only run one pipeline at once, this means that usually you would need to uninstall any active pipelines before replacing it during testing. By default this app is running the the `Morpheus SDK Client` in 'sleep mode' under the `helper` release name. Rather than destroying and recreating the sdk, we will be interfacing with it through the CLI directly, building pipelines with the CLI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Morpheus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morpheus can be run in two ways:\n",
    "\n",
    "- Run off file\n",
    "  \n",
    "    NVIDIA provides several example models and datasets to test morpheus with. You can browse these on their [GitHub](https://github.com/nv-morpheus/Morpheus/tree/branch-22.06/models). The quick start guide includes several examples which involve loading in this data into the input topics and storing outputs in a file. You can also upload your own data to the container using Jupyter's interface.\n",
    "\n",
    "- Run off kafka topic\n",
    "  \n",
    "    NVIDIA Morpheus can interface with the kafka data broker, pulling new data from an input topic and pushing inferences to an output topic. NVIDIA provides scripts to load data from a file into the input topic to simulate a data stream, but in production this should be gathered from the system you are monitoring.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of Morpheus is deployed using four containers:\n",
    "\n",
    "- The *ai-engine* container, which runs NVIDIA Triton, listening for HTTP requests on port 8000, gRPC on port 8001 and metrics in the prometheus format on port 8002.\n",
    "- The *sdk-cli-helper* container, which runs the SDK and the Jupyter Lab (this container). We are using NginX to manage the connections to Jupyter to make your life easier.\n",
    "- The *broker* container, which runs Kafka, listening on port 9092. It is also exposed on 30092 on the main machine allowing you to feed it data, though it will be blocked by default by Nectar's security. You will need to enable this security rule if you'd like to do this, ensure that you are not working with sensitive data if you'd like to experiment with this.\n",
    "- The zookeeper\n",
    "- The *mlflow* container, which runs MLFlow, listening on port 5000. It's UI is also exposed on 30500 on the main machine allowing you to view the models you have deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up some aliases to execute commands in the containers to make our lives easier\n",
    "x_sdk = \"microk8s kubectl -n morpheus exec -it sdk-cli-helper -- \"\n",
    "x_morpheus = \"microk8s kubectl -n morpheus exec -it sdk-cli-helper -- /opt/conda/envs/morpheus/bin/morpheus\"\n",
    "x_broker = \"microk8s kubectl -n morpheus exec deploy/broker -c broker -- \"\n",
    "x_mlflow_python = \"microk8s kubectl -n morpheus exec -it deploy/mlflow -- /opt/conda/envs/mlflow/bin/python\"\n",
    "x_mlflow = \"microk8s kubectl -n morpheus exec -it deploy/mlflow -- /opt/conda/envs/mlflow/bin/mlflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_sdk curl -v ai-engine:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_morpheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_broker kafka-topics.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Information Detection (SID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA has provided us with a set of example models and datasets to get started in the `/models` and `/models/datasets` directory of the SDK container. In this example we will look at the SID model.\n",
    "\n",
    "To share these files with MLFlow, copy it to the `/common` directory which is mapped to `/opt/morpheus/common` on the host. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_sdk cp -RL /workspace/models /common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLFlow service is used for managing and deploying models. We can use NVIDIA's scripts to deploy models to the Triton ai-engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_mlflow_python publish_model_to_mlflow.py \\\n",
    "      --model_name sid-minibert-onnx \\\n",
    "      --model_directory /common/models/triton-model-repo/sid-minibert-onnx \\\n",
    "      --flavor triton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_mlflow deployments create -t triton \\\n",
    "      --flavor triton \\\n",
    "      --name sid-minibert-onnx \\\n",
    "      -m models:/sid-minibert-onnx/1 \\\n",
    "      -C \"version=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our pipeline we will need to create kafka topics for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an input topic\n",
    "!$x_broker kafka-topics.sh\\\n",
    "      --create \\\n",
    "      --bootstrap-server broker:9092 \\\n",
    "      --replication-factor 1 \\\n",
    "      --partitions 3 \\\n",
    "      --topic input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an output topic\n",
    "!$x_broker kafka-topics.sh\\\n",
    "      --create \\\n",
    "      --bootstrap-server broker:9092 \\\n",
    "      --replication-factor 1 \\\n",
    "      --partitions 3 \\\n",
    "      --topic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the topics we've just created\n",
    "!$x_broker --list  --zookeeper zookeeper:2181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_sdk ls examples/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pipeline examples\n",
    "Now that we've finished setting up, we can set up some pipelines! Morpheus constructs pipelines made up of 'stages', including preprocessing and postprocessing steps accellerated with NVIDIA RAPIDS, which will allow you to build something to handle your data stream in near real time. If Morpheus's provided pipeline stages do not fit your needs, it also allows you to extend its capabilities with a [custom stage written in Python or C++](https://docs.nvidia.com/morpheus/developer_guide/guides/1_simple_python_stage.html#background).\n",
    "\n",
    "Morpheus provides scripts to simulate an input stream from a file or by streaming a file into the data broker, but in a production setting you would feed your data into the pipeline using the input topic we set up earlier.\n",
    "\n",
    "For more examples of these pipelines have a read through these [example workflows](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html#example-workflows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a datastream using the pcap_dump dataset file\n",
    "!$x_morpheus --log_level=DEBUG run \\\n",
    "      --num_threads=3 \\\n",
    "      --edge_buffer_size=4 \\\n",
    "      --use_cpp=True \\\n",
    "      --pipeline_batch_size=1024 \\\n",
    "      --model_max_batch_size=32 \\\n",
    "      pipeline-nlp \\\n",
    "        --model_seq_length=256 \\\n",
    "        from-file --filename=./examples/data/pcap_dump.jsonlines \\\n",
    "        monitor --description 'FromFile Rate' --smoothing=0.001 \\\n",
    "        deserialize \\\n",
    "        preprocess --vocab_hash_file=data/bert-base-uncased-hash.txt --truncation=True --do_lower_case=True --add_special_tokens=False \\\n",
    "        monitor --description='Preprocessing rate' \\\n",
    "        inf-triton --force_convert_inputs=True --model_name=sid-minibert-onnx --server_url=ai-engine:8001 \\\n",
    "        monitor --description='Inference rate' --smoothing=0.001 --unit inf \\\n",
    "        add-class \\\n",
    "        serialize --exclude '^ts_' \\\n",
    "        to-file --filename=/common/data/sid-minibert-onnx-output.jsonlines --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Using NVIDIA's pretrained models and examples are a good place to start to get a grasp of the framework, but fine tuning is necessary for any model deployed in a production setting.\n",
    "\n",
    "NVIDIA has provided training scripts and notebooks for each of their models for you to play around with to retrain their models. If their models do not adapt well to your use case you can also look at [hugging face's model repository](https://huggingface.co/models) as a place to start and train your own model.\n",
    "\n",
    "### WARNING\n",
    "Note that many of these scripts and notebooks were moved from their original repositories when being compiled into the Morpheus container. This means that any paths present within these scripts and notebooks **will not accurately reflect where they are in the container**. NVIDIA also **has not provided any environment files** to help us recreate the environments used to train these models. The datasets will likely be somewhere in the folder we copied to `/opt/morpheus/common/` earlier. It may be easier to browse the file structure on the [NVIDIA Morpheus GitHub](https://github.com/nv-morpheus/Morpheus/tree/branch-22.09/models). It will be up to you track down the datasets and recreate the environments if you wish to use the tools that NVIDIA have provided as a starting ground. Alternatively you can look through these resources to get inspiration on how you can train your own models to make use of this framework. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy NVIDIA's training scripts into our root directory so that we can easily access their scripts and notebooks\n",
    "!cp /opt/morpheus/common/models/training-tuning-scripts/ ./ -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "628da563ae30e285400e78415b7ce696a0a3188121a8a2ad023c6c9c424b5215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
