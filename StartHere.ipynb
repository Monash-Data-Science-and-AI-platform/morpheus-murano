{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Morpheus Murano App 1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Murano App uses a microk8s cluster to provide a space to test NVIDIA's Morpheus on the Nectar cloud. To seperate Morpheus from any other deployments you may wish to test, all the `morpheus` namespace has been used. The deployment also includes a jupyter lab server installed with the SDK to make things more user friendly during experimentation.\n",
    "\n",
    "The morpheus stack includes:\n",
    "\n",
    "- [MLFlow](https://mlflow.org/) to manage models\n",
    "- [Triton](https://developer.nvidia.com/nvidia-triton-inference-server) as an inference server\n",
    "- [Kafka](https://kafka.apache.org/) as a data broker\n",
    "\n",
    "You can check the health of the cluster using:\n",
    "`microk8s kubectl -n morpheus get all`\n",
    "\n",
    "If you would like to update Morpheus or for more information on usage, have a read of the [quick start guide](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html#set-up-ngc-api-key-and-install-ngc-registry-cli) which this notebook is based on.\n",
    "\n",
    "NVIDIA recommends that you only run one pipeline at once, this means that you will need to uninstall any active pipelines before replacing it during testing. By default this app is running the the `Morpheus SDK Client` in 'sleep mode' under the `helper` release name. Rather than destroying and recreating the sdk, we will be interfacing with it through the CLI directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             READY   STATUS    RESTARTS      AGE\n",
      "pod/mlflow-7889bfd95f-djsvn      1/1     Running   1 (23h ago)   23h\n",
      "pod/zookeeper-5bdfd5ff4d-8j692   1/1     Running   0             23h\n",
      "pod/broker-6f4d759474-7kn4p      1/1     Running   0             23h\n",
      "pod/ai-engine-868b768b99-fz7c6   1/1     Running   0             23h\n",
      "pod/sdk-cli-helper               1/1     Running   0             22h\n",
      "\n",
      "NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\n",
      "service/mlflow      NodePort    10.152.183.67    <none>        5000:30500/TCP               23h\n",
      "service/zookeeper   ClusterIP   10.152.183.16    <none>        2181/TCP                     23h\n",
      "service/ai-engine   ClusterIP   10.152.183.152   <none>        8000/TCP,8001/TCP,8002/TCP   23h\n",
      "service/broker      NodePort    10.152.183.198   <none>        9092:30092/TCP               23h\n",
      "\n",
      "NAME                        READY   UP-TO-DATE   AVAILABLE   AGE\n",
      "deployment.apps/mlflow      1/1     1            1           23h\n",
      "deployment.apps/zookeeper   1/1     1            1           23h\n",
      "deployment.apps/broker      1/1     1            1           23h\n",
      "deployment.apps/ai-engine   1/1     1            1           23h\n",
      "\n",
      "NAME                                   DESIRED   CURRENT   READY   AGE\n",
      "replicaset.apps/mlflow-7889bfd95f      1         1         1       23h\n",
      "replicaset.apps/zookeeper-5bdfd5ff4d   1         1         1       23h\n",
      "replicaset.apps/broker-6f4d759474      1         1         1       23h\n",
      "replicaset.apps/ai-engine-868b768b99   1         1         1       23h\n"
     ]
    }
   ],
   "source": [
    "!microk8s kubectl -n morpheus get all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Morpheus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Morpheus can be run in two ways:\n",
    "\n",
    "- Run off file\n",
    "  \n",
    "    NVIDIA provides several example models and datasets to test morpheus with. You can browse these on their [GitHub](https://github.com/nv-morpheus/Morpheus/tree/branch-22.06/models). The quick start guide includes several examples which involve loading in this data into the input topics and storing outputs in a file. You can also upload your own data to the container using Jupyter's interface.\n",
    "\n",
    "- Run off incoming packets\n",
    "  \n",
    "    We have provided you with a naiive python script which will scrape any incoming TCP traffic and load it into the `tcpdump_naiive` kafka topic. You will need to open the file and edit the IP to match the address of kafka's bootstrap server(the cluster IP of `service/broker`). By default this will only send the body of the message, though the script also supports sending the rest of the packet information - simply uncomment the relevant line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation of Morpheus is deployed using four containers:\n",
    "\n",
    "- The *ai-engine* container, which runs NVIDIA Triton, listening for HTTP requests on port 8000, gRPC on port 8001 and metrics in the prometheus format on port 8002.\n",
    "- The *sdk-cli-helper* container, which runs the SDK and the Jupyter Lab (this container). We are using NginX to manage the connections to Jupyter to make your life easier.\n",
    "- The *broker* container, which runs Kafka, listening on port 9092. It is also exposed on 30092 on the main machine allowing you to feed it data, though it will be blocked by default by Nectar's security. You will need to enable this security rule if you'd like to do this, ensure that you are not working with sensitive data if you'd like to experiment with this.\n",
    "- The zookeeper\n",
    "- The *mlflow* container, which runs MLFlow, listening on port 5000. It's UI is also exposed on 30500 on the main machine allowing you to view the models you have deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up some aliases to execute commands in the containers to make our lives easier\n",
    "x_sdk = \"microk8s kubectl -n morpheus exec -it sdk-cli-helper -- \"\n",
    "x_morpheus = \"microk8s kubectl -n morpheus exec -it sdk-cli-helper -- /opt/conda/envs/morpheus/bin/morpheus\"\n",
    "x_broker = \"microk8s kubectl -n morpheus exec deploy/broker -c broker -- \"\n",
    "x_mlflow_python = \"microk8s kubectl -n morpheus exec -it deploy/mlflow -- /opt/conda/envs/mlflow/bin/python\"\n",
    "x_mlflow = \"microk8s kubectl -n morpheus exec -it deploy/mlflow -- /opt/conda/envs/mlflow/bin/mlflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*   Trying 10.152.183.152:8000...\n",
      "* Connected to ai-engine (10.152.183.152) port 8000 (#0)\n",
      "> GET /v2/health/ready HTTP/1.1\n",
      "> Host: ai-engine:8000\n",
      "> User-Agent: curl/7.83.1\n",
      "> Accept: */*\n",
      "> \n",
      "* Mark bundle as not supporting multiuse\n",
      "< HTTP/1.1 200 OK\n",
      "< Content-Length: 0\n",
      "< Content-Type: text/plain\n",
      "< \n",
      "* Connection #0 to host ai-engine left intact\n"
     ]
    }
   ],
   "source": [
    "!$x_sdk curl -v ai-engine:8000/v2/health/ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: morpheus [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "Options:\n",
      "  --debug / --no-debug            [default: no-debug]\n",
      "  --log_level [CRITICAL|FATAL|ERROR|WARN|WARNING|INFO|DEBUG]\n",
      "                                  Specify the logging level to use.  [default:\n",
      "                                  WARNING]\n",
      "  --log_config_file FILE          Config file to use to configure logging. Use\n",
      "                                  only for advanced situations. Can accept\n",
      "                                  both JSON and ini style configurations\n",
      "  --version                       Show the version and exit.\n",
      "  --help                          Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  run    Run one of the available pipelines\n",
      "  tools  Run a utility tool\n"
     ]
    }
   ],
   "source": [
    "!$x_morpheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create, delete, describe, or change a topic.\n",
      "Option                                   Description                            \n",
      "------                                   -----------                            \n",
      "--alter                                  Alter the number of partitions,        \n",
      "                                           replica assignment, and/or           \n",
      "                                           configuration for the topic.         \n",
      "--at-min-isr-partitions                  if set when describing topics, only    \n",
      "                                           show partitions whose isr count is   \n",
      "                                           equal to the configured minimum. Not \n",
      "                                           supported with the --zookeeper       \n",
      "                                           option.                              \n",
      "--bootstrap-server <String: server to    REQUIRED: The Kafka server to connect  \n",
      "  connect to>                              to. In case of providing this, a     \n",
      "                                           direct Zookeeper connection won't be \n",
      "                                           required.                            \n",
      "--command-config <String: command        Property file containing configs to be \n",
      "  config property file>                    passed to Admin Client. This is used \n",
      "                                           only with --bootstrap-server option  \n",
      "                                           for describing and altering broker   \n",
      "                                           configs.                             \n",
      "--config <String: name=value>            A topic configuration override for the \n",
      "                                           topic being created or altered. The  \n",
      "                                           following is a list of valid         \n",
      "                                           configurations:                      \n",
      "                                         \tcleanup.policy                        \n",
      "                                         \tcompression.type                      \n",
      "                                         \tdelete.retention.ms                   \n",
      "                                         \tfile.delete.delay.ms                  \n",
      "                                         \tflush.messages                        \n",
      "                                         \tflush.ms                              \n",
      "                                         \tfollower.replication.throttled.       \n",
      "                                           replicas                             \n",
      "                                         \tindex.interval.bytes                  \n",
      "                                         \tleader.replication.throttled.replicas \n",
      "                                         \tmax.compaction.lag.ms                 \n",
      "                                         \tmax.message.bytes                     \n",
      "                                         \tmessage.downconversion.enable         \n",
      "                                         \tmessage.format.version                \n",
      "                                         \tmessage.timestamp.difference.max.ms   \n",
      "                                         \tmessage.timestamp.type                \n",
      "                                         \tmin.cleanable.dirty.ratio             \n",
      "                                         \tmin.compaction.lag.ms                 \n",
      "                                         \tmin.insync.replicas                   \n",
      "                                         \tpreallocate                           \n",
      "                                         \tretention.bytes                       \n",
      "                                         \tretention.ms                          \n",
      "                                         \tsegment.bytes                         \n",
      "                                         \tsegment.index.bytes                   \n",
      "                                         \tsegment.jitter.ms                     \n",
      "                                         \tsegment.ms                            \n",
      "                                         \tunclean.leader.election.enable        \n",
      "                                         See the Kafka documentation for full   \n",
      "                                           details on the topic configs. It is  \n",
      "                                           supported only in combination with --\n",
      "                                           create if --bootstrap-server option  \n",
      "                                           is used (the kafka-configs CLI       \n",
      "                                           supports altering topic configs with \n",
      "                                           a --bootstrap-server option).        \n",
      "--create                                 Create a new topic.                    \n",
      "--delete                                 Delete a topic                         \n",
      "--delete-config <String: name>           A topic configuration override to be   \n",
      "                                           removed for an existing topic (see   \n",
      "                                           the list of configurations under the \n",
      "                                           --config option). Not supported with \n",
      "                                           the --bootstrap-server option.       \n",
      "--describe                               List details for the given topics.     \n",
      "--disable-rack-aware                     Disable rack aware replica assignment  \n",
      "--exclude-internal                       exclude internal topics when running   \n",
      "                                           list or describe command. The        \n",
      "                                           internal topics will be listed by    \n",
      "                                           default                              \n",
      "--force                                  Suppress console prompts               \n",
      "--help                                   Print usage information.               \n",
      "--if-exists                              if set when altering or deleting or    \n",
      "                                           describing topics, the action will   \n",
      "                                           only execute if the topic exists.    \n",
      "--if-not-exists                          if set when creating topics, the       \n",
      "                                           action will only execute if the      \n",
      "                                           topic does not already exist.        \n",
      "--list                                   List all available topics.             \n",
      "--partitions <Integer: # of partitions>  The number of partitions for the topic \n",
      "                                           being created or altered (WARNING:   \n",
      "                                           If partitions are increased for a    \n",
      "                                           topic that has a key, the partition  \n",
      "                                           logic or ordering of the messages    \n",
      "                                           will be affected). If not supplied   \n",
      "                                           for create, defaults to the cluster  \n",
      "                                           default.                             \n",
      "--replica-assignment <String:            A list of manual partition-to-broker   \n",
      "  broker_id_for_part1_replica1 :           assignments for the topic being      \n",
      "  broker_id_for_part1_replica2 ,           created or altered.                  \n",
      "  broker_id_for_part2_replica1 :                                                \n",
      "  broker_id_for_part2_replica2 , ...>                                           \n",
      "--replication-factor <Integer:           The replication factor for each        \n",
      "  replication factor>                      partition in the topic being         \n",
      "                                           created. If not supplied, defaults   \n",
      "                                           to the cluster default.              \n",
      "--topic <String: topic>                  The topic to create, alter, describe   \n",
      "                                           or delete. It also accepts a regular \n",
      "                                           expression, except for --create      \n",
      "                                           option. Put topic name in double     \n",
      "                                           quotes and use the '\\' prefix to     \n",
      "                                           escape regular expression symbols; e.\n",
      "                                           g. \"test\\.topic\".                    \n",
      "--topics-with-overrides                  if set when describing topics, only    \n",
      "                                           show topics that have overridden     \n",
      "                                           configs                              \n",
      "--unavailable-partitions                 if set when describing topics, only    \n",
      "                                           show partitions whose leader is not  \n",
      "                                           available                            \n",
      "--under-min-isr-partitions               if set when describing topics, only    \n",
      "                                           show partitions whose isr count is   \n",
      "                                           less than the configured minimum.    \n",
      "                                           Not supported with the --zookeeper   \n",
      "                                           option.                              \n",
      "--under-replicated-partitions            if set when describing topics, only    \n",
      "                                           show under replicated partitions     \n",
      "--version                                Display Kafka version.                 \n",
      "--zookeeper <String: hosts>              DEPRECATED, The connection string for  \n",
      "                                           the zookeeper connection in the form \n",
      "                                           host:port. Multiple hosts can be     \n",
      "                                           given to allow fail-over.            \n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "!$x_broker kafka-topics.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: mlflow [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "Options:\n",
      "  --version  Show the version and exit.\n",
      "  --help     Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  artifacts    Upload, list, and download artifacts from an MLflow...\n",
      "  azureml      Serve models on Azure ML.\n",
      "  db           Commands for managing an MLflow tracking database.\n",
      "  deployments  Deploy MLflow models to custom targets.\n",
      "  experiments  Manage experiments.\n",
      "  gc           Permanently delete runs in the `deleted` lifecycle stage.\n",
      "  models       Deploy MLflow models locally.\n",
      "  run          Run an MLflow project from the given URI.\n",
      "  runs         Manage runs.\n",
      "  sagemaker    Serve models on SageMaker.\n",
      "  server       Run the MLflow tracking server.\n",
      "  ui           Launch the MLflow tracking UI for local viewing of run...\n"
     ]
    }
   ],
   "source": [
    "!$x_mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sensitive Information Detection (SID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NVIDIA has provided us with a set of example models and datasets to get started in the `/models` and `/models/datasets` directory of the SDK container. In this example we will look at the SID model.\n",
    "\n",
    "To share these files with MLFlow, copy it to the `/common` directory which is mapped to `/opt/morpheus/common` on the host. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!$x_sdk cp -RL /workspace/models /common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLFlow service is used for managing and deploying models. We can use NVIDIA's scripts to deploy models to the Triton ai-engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model 'sid-minibert-onnx' already exists. Creating a new version of this model...\n",
      "2022/09/20 05:16:23 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: sid-minibert-onnx, version 4\n",
      "Created version '4' of model 'sid-minibert-onnx'.\n",
      "/mlflow/artifacts/0/c7afb7d6e5014f3b8fed303effb2c8f3/artifacts\n"
     ]
    }
   ],
   "source": [
    "!$x_mlflow_python publish_model_to_mlflow.py \\\n",
    "      --model_name sid-minibert-onnx \\\n",
    "      --model_directory /common/models/triton-model-repo/sid-minibert-onnx \\\n",
    "      --flavor triton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/mlflow/bin/mlflow\", line 8, in <module>\n",
      "    sys.exit(cli())\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1053, in main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/click/core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/mlflow/deployments/cli.py\", line 133, in create_deployment\n",
      "    deployment = client.create_deployment(name, model_uri, flavor, config=config_dict)\n",
      "  File \"/opt/conda/envs/mlflow/lib/python3.8/site-packages/mlflow_triton/deployments.py\", line 99, in create_deployment\n",
      "    raise Exception(\n",
      "Exception: Unable to create deployment for name sid-minibert-onnx because it already exists.\n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "!$x_mlflow deployments create -t triton \\\n",
    "      --flavor triton \\\n",
    "      --name sid-minibert-onnx \\\n",
    "      -m models:/sid-minibert-onnx/1 \\\n",
    "      -C \"version=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our pipeline we will need to create kafka topics for input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while executing topic command : Topic 'input' already exists.\n",
      "[2022-09-20 05:27:45,706] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'input' already exists.\n",
      " (kafka.admin.TopicCommand$)\n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "# Create an input topic\n",
    "!$x_broker kafka-topics.sh\\\n",
    "      --create \\\n",
    "      --bootstrap-server broker:9092 \\\n",
    "      --replication-factor 1 \\\n",
    "      --partitions 3 \\\n",
    "      --topic input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error while executing topic command : Topic 'output' already exists.\n",
      "[2022-09-20 05:27:48,438] ERROR org.apache.kafka.common.errors.TopicExistsException: Topic 'output' already exists.\n",
      " (kafka.admin.TopicCommand$)\n",
      "command terminated with exit code 1\n"
     ]
    }
   ],
   "source": [
    "# Create an output topic\n",
    "!$x_broker kafka-topics.sh\\\n",
    "      --create \\\n",
    "      --bootstrap-server broker:9092 \\\n",
    "      --replication-factor 1 \\\n",
    "      --partitions 3 \\\n",
    "      --topic output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__consumer_offsets\n",
      "input\n",
      "output\n"
     ]
    }
   ],
   "source": [
    "# View the topics we've just created\n",
    "!$x_broker --list  --zookeeper zookeeper:2181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abp_pcap_dump.jsonlines\t\tnvsmi.jsonlines\n",
      "appshield\t\t\tpcap_dump.jsonlines\n",
      "email.jsonlines\t\t\tsid_training_data_truth.csv\n",
      "email_with_addresses.jsonlines\n"
     ]
    }
   ],
   "source": [
    "!$x_sdk ls examples/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Pipeline examples\n",
    "Now that we've finished setting up, we can set up some pipelines! Morpheus constructs pipelines made up of 'stages', including preprocessing and postprocessing steps accellerated with NVIDIA RAPIDS, which will allow you to build something to handle your data stream in near real time. If Morpheus's provided pipeline stages do not fit your needs, it also allows you to extend its capabilities with a [custom stage written in Python or C++](https://docs.nvidia.com/morpheus/developer_guide/guides/1_simple_python_stage.html#background).\n",
    "\n",
    "Morpheus provides scripts to simulate an input stream from a file or by streaming a file into the data broker, but in a production setting you would feed your data into the pipeline using the input topic we set up earlier.\n",
    "\n",
    "For more examples of these pipelines have a read through these [example workflows](https://docs.nvidia.com/morpheus/morpheus_quickstart_guide.html#example-workflows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mParameter, 'labels_file', with relative path, 'data/labels_nlp.txt', does not exist. Using package relative location: '/opt/conda/envs/morpheus/lib/python3.8/site-packages/morpheus/data/labels_nlp.txt'\u001b[0m\n",
      "\u001b[32mConfiguring Pipeline via CLI\u001b[0m\n",
      "\u001b[2mLoaded labels file. Current labels: [['address', 'bank_acct', 'credit_card', 'email', 'govt_id', 'name', 'password', 'phone_num', 'secret_keys', 'user']]\u001b[0m\n",
      "\u001b[2mParameter, 'vocab_hash_file', with relative path, 'data/bert-base-uncased-hash.txt', does not exist. Using package relative location: '/opt/conda/envs/morpheus/lib/python3.8/site-packages/morpheus/data/bert-base-uncased-hash.txt'\u001b[0m\n",
      "\u001b[31mStarting pipeline via CLI... Ctrl+C to Quit\u001b[0m\n",
      "Config: \n",
      "{\n",
      "  \"ae\": null,\n",
      "  \"class_labels\": [\n",
      "    \"address\",\n",
      "    \"bank_acct\",\n",
      "    \"credit_card\",\n",
      "    \"email\",\n",
      "    \"govt_id\",\n",
      "    \"name\",\n",
      "    \"password\",\n",
      "    \"phone_num\",\n",
      "    \"secret_keys\",\n",
      "    \"user\"\n",
      "  ],\n",
      "  \"debug\": false,\n",
      "  \"edge_buffer_size\": 4,\n",
      "  \"feature_length\": 256,\n",
      "  \"fil\": null,\n",
      "  \"log_config_file\": null,\n",
      "  \"log_level\": 10,\n",
      "  \"mode\": \"NLP\",\n",
      "  \"model_max_batch_size\": 32,\n",
      "  \"num_threads\": 3,\n",
      "  \"pipeline_batch_size\": 1024\n",
      "}\u001b[0m\n",
      "CPP Enabled: True\u001b[0m\n",
      "====Registering Pipeline====\u001b[0m\n",
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "W20220920 07:20:51.882804  2184 triton_inference.cpp:248] Failed to connect to Triton at 'ai-engine:8001'. Default gRPC port of (8001) was detected but C++ InferenceClientStage uses HTTP protocol. Retrying with default HTTP port (8000)\n",
      "====Registering Pipeline Complete!====\u001b[0m\n",
      "FromFile Rate: 0messages [00:00, ?messages/s]\n",
      "Preprocessing rate: 0messages [00:00, ?messages/s]\u001b[A\n",
      "Preprocessing rate: 0messages [00:00, ?messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 0inf [00:00, ?inf/s]\u001b[A\u001b[A\n",
      "\n",
      "Inference rate: 0inf [00:00, ?inf/s]\u001b[A\u001b[A====Starting Pipeline====\u001b[0m\n",
      "====Pipeline Started====\u001b[0m\n",
      "====Building Pipeline====\u001b[0m\n",
      "Added source: <from-file-0; FileSourceStage(filename=./examples/data/pcap_dump.jsonlines, iterative=False, file_type=FileTypes.Auto, repeat=1, filter_null=True, cudf_kwargs=None)>\n",
      "  └─> morpheus.MessageMeta\u001b[0m\n",
      "Added stage: <monitor-1; MonitorStage(description=FromFile Rate, smoothing=0.001, unit=messages, delayed_start=False, determine_count_fn=None)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n",
      "Added stage: <deserialize-2; DeserializeStage()>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MultiMessage\u001b[0m\n",
      "Added stage: <preprocess-nlp-3; PreprocessNLPStage(vocab_hash_file=/opt/conda/envs/morpheus/lib/python3.8/site-packages/morpheus/data/bert-base-uncased-hash.txt, truncation=True, do_lower_case=True, add_special_tokens=False, stride=-1)>\n",
      "  └─ morpheus.MultiMessage -> morpheus.MultiInferenceNLPMessage\u001b[0m\n",
      "Added stage: <monitor-4; MonitorStage(description=Preprocessing rate, smoothing=0.05, unit=messages, delayed_start=False, determine_count_fn=None)>\n",
      "  └─ morpheus.MultiInferenceNLPMessage -> morpheus.MultiInferenceNLPMessage\u001b[0m\n",
      "Added stage: <inference-5; TritonInferenceStage(model_name=sid-minibert-onnx, server_url=ai-engine:8001, force_convert_inputs=True, use_shared_memory=False)>\n",
      "  └─ morpheus.MultiInferenceNLPMessage -> morpheus.MultiResponseProbsMessage\u001b[0m\n",
      "Added stage: <monitor-6; MonitorStage(description=Inference rate, smoothing=0.001, unit=inf, delayed_start=False, determine_count_fn=None)>\n",
      "  └─ morpheus.MultiResponseProbsMessage -> morpheus.MultiResponseProbsMessage\u001b[0m\n",
      "Added stage: <add-class-7; AddClassificationsStage(threshold=0.5, labels=[], prefix=)>\n",
      "  └─ morpheus.MultiResponseProbsMessage -> morpheus.MultiResponseProbsMessage\u001b[0m\n",
      "Added stage: <serialize-8; SerializeStage(include=[], exclude=['^ts_'], fixed_columns=True)>\n",
      "  └─ morpheus.MultiResponseProbsMessage -> morpheus.MessageMeta\u001b[0m\n",
      "Added stage: <to-file-9; WriteToFileStage(filename=/common/data/sid-minibert-onnx-output.jsonlines, overwrite=True, file_type=FileTypes.Auto)>\n",
      "  └─ morpheus.MessageMeta -> morpheus.MessageMeta\u001b[0m\n",
      "====Building Pipeline Complete!====\u001b[0m\n",
      "\u001b[2mStarting! Time: 1663658451.8958783\u001b[0m\n",
      "FromFile Rate[Complete]: 93085messages [00:01, 83015.30messages/s]\n",
      "Preprocessing rate: 1024messages [00:01, 897.39messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 1024inf [00:01, 573.83inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 8192messages [00:01, 4709.73messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 4096inf [00:02, 1726.46inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 11264messages [00:02, 4842.70messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 7168inf [00:02, 2430.61inf/s]\u001b[A\u001b[A\n",
      "FromFile Rate[Complete]: 93085messages [00:01, 83015.30messages/s]\n",
      "\n",
      "Inference rate: 10240inf [00:03, 2936.44inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 17408messages [00:03, 5075.95messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 12288inf [00:03, 3232.33inf/s]\u001b[A\u001b[A\n",
      "FromFile Rate[Complete]: 93085messages [00:01, 83015.30messages/s]\n",
      "\n",
      "Inference rate: 14336inf [00:04, 3484.52inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 21504messages [00:04, 5335.68messages/s]\u001b[A\n",
      "Preprocessing rate: 22528messages [00:04, 5059.94messages/s]\u001b[A\n",
      "\n",
      "FromFile Rate[Complete]: 93085messages [00:01, 83015.30messages/s]\n",
      "\n",
      "Inference rate: 16384inf [00:05, 3245.62inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 25600messages [00:05, 4994.07messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 17408inf [00:05, 3191.05inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 27648messages [00:05, 5125.01messages/s]\u001b[A\n",
      "\n",
      "Inference rate: 18432inf [00:05, 3217.98inf/s]\u001b[A\u001b[A\n",
      "Preprocessing rate: 28672messages [00:05, 5020.78messages/s]\u001b[A^C\n",
      "                                                                  \n",
      "\u001b[A                                                         \n",
      "\n",
      "\u001b[A\u001b[AStopping pipeline. Please wait... Press Ctrl+C again to kill.\n",
      "FromFile Rate[Complete]: 93085messages [00:01, 83015.30messages/s]\n",
      "Preprocessing rate: 28672messages [00:05, 5020.78messages/s]\u001b[A\n",
      "\n",
      "FromFile Rate[Complete]: 93085messages [00:01, 83016.19messages/s]\n",
      "Preprocessing rate: 28672messages [00:05, 4842.96messages/s]\n",
      "====Stopping Pipeline====\u001b[0m\n",
      "Inference rate: 18432inf [00:05, 3112.90inf/s]\n",
      "====Pipeline Stopped====\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Simulate a datastream using the pcap_dump dataset file\n",
    "!$x_morpheus --log_level=DEBUG run \\\n",
    "      --num_threads=3 \\\n",
    "      --edge_buffer_size=4 \\\n",
    "      --use_cpp=True \\\n",
    "      --pipeline_batch_size=1024 \\\n",
    "      --model_max_batch_size=32 \\\n",
    "      pipeline-nlp \\\n",
    "        --model_seq_length=256 \\\n",
    "        from-file --filename=./examples/data/pcap_dump.jsonlines \\\n",
    "        monitor --description 'FromFile Rate' --smoothing=0.001 \\\n",
    "        deserialize \\\n",
    "        preprocess --vocab_hash_file=data/bert-base-uncased-hash.txt --truncation=True --do_lower_case=True --add_special_tokens=False \\\n",
    "        monitor --description='Preprocessing rate' \\\n",
    "        inf-triton --force_convert_inputs=True --model_name=sid-minibert-onnx --server_url=ai-engine:8001 \\\n",
    "        monitor --description='Inference rate' --smoothing=0.001 --unit inf \\\n",
    "        add-class \\\n",
    "        serialize --exclude '^ts_' \\\n",
    "        to-file --filename=/common/data/sid-minibert-onnx-output.jsonlines --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Using NVIDIA's pretrained models and examples are a good place to start to get a grasp of the framework, but fine tuning is necessary for any model deployed in a production setting.\n",
    "\n",
    "NVIDIA has provided training scripts and notebooks for each of their models for you to play around with to retrain their models. If their models do not adapt well to your use case you can also look at [hugging face's model repository](https://huggingface.co/models) as a place to start and train your own model.\n",
    "\n",
    "### WARNING\n",
    "Note that many of these scripts and notebooks were moved from their original repositories when being compiled into the Morpheus container. This means that any paths present within these scripts and notebooks **will not accurately reflect where they are in the container**. NVIDIA also **has not provided any environment files** to help us recreate the environments used to train these models. The datasets will likely be somewhere in the folder we copied to `/opt/morpheus/common/` earlier. It may be easier to browse the file structure on the [NVIDIA Morpheus GitHub](https://github.com/nv-morpheus/Morpheus/tree/branch-22.09/models). It will be up to you track down the datasets and recreate the environments if you wish to use the tools that NVIDIA have provided as a starting ground. Alternatively you can look through these resources to get inspiration on how you can train your own models to make use of this framework. \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy NVIDIA's training scripts into our root directory so that we can easily access their scripts and notebooks\n",
    "!cp /opt/morpheus/common/models/training-tuning-scripts/ ./ -r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "628da563ae30e285400e78415b7ce696a0a3188121a8a2ad023c6c9c424b5215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
